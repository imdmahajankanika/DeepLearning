{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kanika_Mahajan_week4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g2vm0mKNv9Bt",
        "PhZCIS71echa",
        "0w49TdXZfZuZ",
        "cMeT7HXzaeu7",
        "V2DGG0HecY23",
        "weQw4W3vcbMO",
        "zrYRcpzxe9O6",
        "-KfH692UmEQe",
        "YeIUTU4osp7n",
        "dLkixv4zwhPf"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOnf7RiCEXNhO0kUB1TRcjK"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2vm0mKNv9Bt"
      },
      "source": [
        "#IMDB DataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTBe5kPpANbL"
      },
      "source": [
        "#Exe. 1 Load IMDB dataset from Keras \n",
        "with maximum number of num words =\n",
        "10000 to include. It is a large dataset containing the text of 50, 000 movie reviews from the Internet Movie Database. This is a dataset for binary sentiment\n",
        "classification of movies containing {0, 1}. Check train and test datasets sizes\n",
        "and shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNODBvAR_K1K"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynCTpLrnAsrs",
        "outputId": "35e1278a-1519-4281-e9f1-702743ac6b0a"
      },
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwkwPYe9CQox",
        "outputId": "28e2b6d0-78da-408f-d16d-d1474906d4ab"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fXiJiiZCUhc",
        "outputId": "1244285d-73a6-4bbd-b07f-ec3d0ca43929"
      },
      "source": [
        "print(test_data[0])\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 7944, 451, 202, 14, 6, 717]\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2NyBr_AEBL8"
      },
      "source": [
        "#Exe. 2 Prepare the data using the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWRUil33EK9Q"
      },
      "source": [
        "def vectorize_sequences ( sequences , dimension =10000):\n",
        "# Create an all - zero matrix of shape ( len ( sequences ), dimension)\n",
        "  results = np.zeros((len( sequences ) , dimension))\n",
        "  #print(results.shape)\n",
        "  for i , sequence in enumerate (sequences):\n",
        "    results [i , sequence ] = 1. # set specific indices of results [i] to 1s\n",
        "  return results\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data )\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)\n",
        "  # Our vectorized labels\n",
        "y_train = np.asarray( train_labels ).astype ('float32')\n",
        "y_test = np.asarray( test_labels ).astype ('float32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQpO2vPtU7J-",
        "outputId": "56c3bfc2-ed6a-440c-ccc7-77f0a54763fe"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 10000)\n",
            "(25000, 10000)\n",
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-kgrz4qVqqp"
      },
      "source": [
        "We can observe that, input data is vectorized now (previously it was a data structure, not a vector) that is, now input data has 25000 rows and 10000 columns for both x_train and x_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaDrwk5JYhWN"
      },
      "source": [
        "#Exe. 3 reducing the network size\n",
        "design a neural network model namely\n",
        "**original_model** with **two dense layers** of size **16** and each followed with an\n",
        "activation function **relu**. Add a **final layer of 1 node** and an activation function\n",
        "of **sigmoid**. For your network compilation, define a **rmsprop optimizer**, with a\n",
        "**binary_crossentropy** and **acc** as the metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKn_k_0sYjos"
      },
      "source": [
        "original_model = keras.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "original_model.compile(optimizer ='rmsprop',loss ='binary_crossentropy',metrics =['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "repsgR2sbrGi"
      },
      "source": [
        "#Exe. 4 Design a neural network model namely smaller_model\n",
        "with **two dense layers** of size **4** and each followed with an activation function **relu**. Add a **final layer of 1 node **and an activation function of **sigmoid**. For your network compilation, define a **rmsprop** optimizer, with a **binary_crossentropy** and **acc** as\n",
        "the metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMLQ8ZeubwOD"
      },
      "source": [
        "smaller_model = keras.Sequential([\n",
        "    keras.layers.Dense(4, activation='relu'),\n",
        "    keras.layers.Dense(4, activation='relu'),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "smaller_model.compile(optimizer ='rmsprop',loss ='binary_crossentropy',metrics =['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5jKKyazcEAH"
      },
      "source": [
        "#Exe. 5 \n",
        "Fit the original model on the train set with the test set as the\n",
        "validation set, 20 epochs and batch size = 512 and save it in original hist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6368QJtcfOK",
        "outputId": "38eff2b4-68f6-4642-ec65-1c52e2d76eec"
      },
      "source": [
        " original_hist = original_model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs =20,batch_size = 512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 4s 56ms/step - loss: 0.5306 - acc: 0.7481 - val_loss: 0.3337 - val_acc: 0.8728\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.2593 - acc: 0.9104 - val_loss: 0.3053 - val_acc: 0.8764\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.1945 - acc: 0.9337 - val_loss: 0.2901 - val_acc: 0.8838\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1596 - acc: 0.9453 - val_loss: 0.2966 - val_acc: 0.8832\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1324 - acc: 0.9565 - val_loss: 0.3317 - val_acc: 0.8749\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1186 - acc: 0.9607 - val_loss: 0.3375 - val_acc: 0.8756\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.1018 - acc: 0.9681 - val_loss: 0.3747 - val_acc: 0.8682\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.0867 - acc: 0.9725 - val_loss: 0.3893 - val_acc: 0.8676\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.0725 - acc: 0.9776 - val_loss: 0.4346 - val_acc: 0.8616\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.0670 - acc: 0.9806 - val_loss: 0.4384 - val_acc: 0.8658\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.0532 - acc: 0.9846 - val_loss: 0.4711 - val_acc: 0.8628\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.0455 - acc: 0.9877 - val_loss: 0.5200 - val_acc: 0.8575\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.0384 - acc: 0.9899 - val_loss: 0.5463 - val_acc: 0.8575\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.0305 - acc: 0.9931 - val_loss: 0.5921 - val_acc: 0.8555\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.0256 - acc: 0.9942 - val_loss: 0.6218 - val_acc: 0.8558\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.0201 - acc: 0.9957 - val_loss: 0.6632 - val_acc: 0.8536\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.0165 - acc: 0.9963 - val_loss: 0.7119 - val_acc: 0.8522\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.0142 - acc: 0.9967 - val_loss: 0.7583 - val_acc: 0.8500\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.0116 - acc: 0.9974 - val_loss: 0.8034 - val_acc: 0.8503\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.0093 - acc: 0.9983 - val_loss: 0.8575 - val_acc: 0.8480\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhZCIS71echa"
      },
      "source": [
        "#Exe. 6\n",
        "Fit the smaller model on the train set with the test set as the validation set,\n",
        "20 epochs and batch size = 512 and save it in smaller hist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ocVe-_deXYY",
        "outputId": "61fd9f3a-efe7-4f0a-b83f-8719a4c400ec"
      },
      "source": [
        "smaller_hist = smaller_model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20,batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6335 - acc: 0.6080 - val_loss: 0.5344 - val_acc: 0.7907\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 0.4975 - acc: 0.8457 - val_loss: 0.4687 - val_acc: 0.8696\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 0.4204 - acc: 0.9058 - val_loss: 0.4163 - val_acc: 0.8858\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.3428 - acc: 0.9281 - val_loss: 0.3315 - val_acc: 0.8830\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.2420 - acc: 0.9388 - val_loss: 0.2908 - val_acc: 0.8884\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.1898 - acc: 0.9452 - val_loss: 0.3021 - val_acc: 0.8782\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.1670 - acc: 0.9494 - val_loss: 0.2857 - val_acc: 0.8856\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.1474 - acc: 0.9560 - val_loss: 0.2935 - val_acc: 0.8832\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.1348 - acc: 0.9589 - val_loss: 0.3046 - val_acc: 0.8804\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.1201 - acc: 0.9634 - val_loss: 0.3284 - val_acc: 0.8742\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.1107 - acc: 0.9675 - val_loss: 0.3292 - val_acc: 0.8746\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.1015 - acc: 0.9691 - val_loss: 0.3540 - val_acc: 0.8697\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.0971 - acc: 0.9715 - val_loss: 0.3558 - val_acc: 0.8729\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0881 - acc: 0.9750 - val_loss: 0.3806 - val_acc: 0.8688\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.0825 - acc: 0.9754 - val_loss: 0.3908 - val_acc: 0.8676\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0709 - acc: 0.9796 - val_loss: 0.4188 - val_acc: 0.8638\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.0684 - acc: 0.9817 - val_loss: 0.4343 - val_acc: 0.8632\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.0597 - acc: 0.9843 - val_loss: 0.4488 - val_acc: 0.8636\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0539 - acc: 0.9862 - val_loss: 0.4745 - val_acc: 0.8608\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 36ms/step - loss: 0.0525 - acc: 0.9861 - val_loss: 0.4980 - val_acc: 0.8590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w49TdXZfZuZ"
      },
      "source": [
        "#Exe. 7 \n",
        "Get the val_loss from the trained model histogram and save them in\n",
        "new variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xMOnPY7ebI_"
      },
      "source": [
        "# validation loss\n",
        "original_loss = original_hist.history['val_loss']\n",
        "smaller_loss = smaller_hist.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcT1B6IrUKhg"
      },
      "source": [
        "#Exe. 8 Plot the validation loss values w.r.t the epochs\n",
        "(we have 20 epochs)\n",
        "and observe the loss value changing through the epochs increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "rMOa7MK7UPlO",
        "outputId": "096e7f96-7229-4aef-c66e-e2ea02a3949f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epochs = range(1,21)\n",
        "plt.plot(epochs, original_loss, 'b', linestyle='', marker='+',label='Original model')\n",
        "plt.plot(epochs, smaller_loss, 'b', linestyle='', marker='o', label='Smaller model')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIl6heuJZUOw"
      },
      "source": [
        "We can observe that for initially with increase in epochs the validation loss for both original and smaller model decreased till the value 5.0, but after epoch 5.0, validation loss for original model increased rapidly and for smaller model it increased but at the lower rate.\n",
        "\n",
        "Thus, smaller model has mitigated the overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMeT7HXzaeu7"
      },
      "source": [
        "#Exe. 9 Design a network that has much more capacity, far more than the problem would warrant. \n",
        "Call this model bigger_model with two dense layers of size 512 and each followed with an activation function relu. Add a final layer of 1\n",
        "node and an activation function of sigmoid. For your network compilation, define a rmsprop optimizer, with a binary_crossentropy and acc as the metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doge6myNbg19"
      },
      "source": [
        "bigger_model = keras.Sequential([\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "bigger_model.compile(optimizer ='rmsprop',loss ='binary_crossentropy',metrics =['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2DGG0HecY23"
      },
      "source": [
        "#Exe. 10 Fit the bigger_model on the train set with the test set as the validation set, 20 epochs and batch size = 512 and save it in bigger_hist.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4wAZouvcdhP",
        "outputId": "6ed00432-f3c1-456c-90a1-e5785c922e6c"
      },
      "source": [
        " bigger_hist = bigger_model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs =20,batch_size = 512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 17s 339ms/step - loss: 0.5999 - acc: 0.7251 - val_loss: 0.3528 - val_acc: 0.8486\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 16s 336ms/step - loss: 0.2253 - acc: 0.9115 - val_loss: 0.3235 - val_acc: 0.8669\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 17s 339ms/step - loss: 0.1323 - acc: 0.9494 - val_loss: 0.4129 - val_acc: 0.8540\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 17s 344ms/step - loss: 0.0561 - acc: 0.9819 - val_loss: 0.9115 - val_acc: 0.7880\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 16s 337ms/step - loss: 0.0344 - acc: 0.9908 - val_loss: 0.4426 - val_acc: 0.8755\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 17s 340ms/step - loss: 0.0037 - acc: 0.9999 - val_loss: 0.7128 - val_acc: 0.8801\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 17s 343ms/step - loss: 3.1599e-04 - acc: 1.0000 - val_loss: 0.9092 - val_acc: 0.8786\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 17s 351ms/step - loss: 0.0313 - acc: 0.9981 - val_loss: 0.8415 - val_acc: 0.8703\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 17s 343ms/step - loss: 7.7546e-05 - acc: 1.0000 - val_loss: 0.8448 - val_acc: 0.8759\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 17s 344ms/step - loss: 1.6581e-05 - acc: 1.0000 - val_loss: 0.9649 - val_acc: 0.8761\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 17s 347ms/step - loss: 5.9282e-06 - acc: 1.0000 - val_loss: 1.1074 - val_acc: 0.8768\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 17s 343ms/step - loss: 1.0026e-06 - acc: 1.0000 - val_loss: 1.2520 - val_acc: 0.8770\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 17s 339ms/step - loss: 1.8587e-07 - acc: 1.0000 - val_loss: 1.4050 - val_acc: 0.8771\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 18s 360ms/step - loss: 5.0124e-08 - acc: 1.0000 - val_loss: 1.4678 - val_acc: 0.8781\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 17s 346ms/step - loss: 2.2215e-08 - acc: 1.0000 - val_loss: 1.5109 - val_acc: 0.8776\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 17s 346ms/step - loss: 1.3764e-08 - acc: 1.0000 - val_loss: 1.5505 - val_acc: 0.8778\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 16s 337ms/step - loss: 9.7193e-09 - acc: 1.0000 - val_loss: 1.5709 - val_acc: 0.8778\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 16s 337ms/step - loss: 7.8569e-09 - acc: 1.0000 - val_loss: 1.5860 - val_acc: 0.8778\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 16s 336ms/step - loss: 7.0003e-09 - acc: 1.0000 - val_loss: 1.6032 - val_acc: 0.8776\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 17s 339ms/step - loss: 5.2888e-09 - acc: 1.0000 - val_loss: 1.6126 - val_acc: 0.8775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87PoZ67JdTfV"
      },
      "source": [
        "#Exe. 11 Plot the bigger and original validation loss changing w.r.t epochs changing. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OVPrJAxUhMpK",
        "outputId": "116a8aac-7b49-4d1d-8088-97459ca82c02"
      },
      "source": [
        "bigger_loss=bigger_hist.history['val_loss']\n",
        "epochs = range(1,21)\n",
        "plt.plot(epochs, bigger_loss, 'b', linestyle='', marker='+',label='Bigger model')\n",
        "plt.plot(epochs, original_loss, 'b', linestyle='', marker='o', label='Original model')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkOna-0diDHx"
      },
      "source": [
        "The more capacity the network has, the quicker it\n",
        "will be able to model the training data, but if it converges quickly to 0, this is more susceptible for overfitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weQw4W3vcbMO"
      },
      "source": [
        "# Weight Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6qIdQbbcgUD"
      },
      "source": [
        "A common way to mitigate overfitting is to put constraints on the\n",
        "complexity of a network by forcing its weights to only take small values, which\n",
        "makes the distribution of weight values more ”regular”. This is called  **\"weight\n",
        "regularization\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoJC1gvcc688"
      },
      "source": [
        "In Keras, weight regularization is added by passing weight regularizer instances\n",
        "to layers as keyword arguments: for instance,  **kernel regularizer=regularizers.l1(0.001)** where l1(0.001) means that every coefficient in the\n",
        "weight matrix of the layer will add  **0.001 * weight coefficient** value to the total\n",
        "loss of the network. Note that because this penalty is only added at training\n",
        "time, the loss for this network will be much higher at training than at test time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrYRcpzxe9O6"
      },
      "source": [
        "#Exe. 12 Modify the original network as l2_model by adding an L2 weight regularization to the model1. \n",
        "Add the L2 regularizers to the first two layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "caTKrubcfKme"
      },
      "source": [
        "from keras import regularizers\n",
        "l2_model=keras.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
        "    keras.layers.Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.001)),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "\n",
        "l2_model.compile(optimizer ='rmsprop',loss ='binary_crossentropy',metrics =['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KfH692UmEQe"
      },
      "source": [
        "#Exe. 13 Fit the model on train set with previously presented parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nHK4hX22mIet",
        "outputId": "c79cb758-68f6-4a41-c3fb-ef7d19ea961e"
      },
      "source": [
        " l2_model_hist = l2_model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs =20,batch_size = 512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 51ms/step - loss: 0.5958 - acc: 0.7430 - val_loss: 0.3869 - val_acc: 0.8812\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.3315 - acc: 0.9028 - val_loss: 0.3357 - val_acc: 0.8906\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.2713 - acc: 0.9247 - val_loss: 0.3679 - val_acc: 0.8680\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.2480 - acc: 0.9315 - val_loss: 0.3659 - val_acc: 0.8722\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.2351 - acc: 0.9349 - val_loss: 0.3410 - val_acc: 0.8854\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.2264 - acc: 0.9399 - val_loss: 0.3763 - val_acc: 0.8740\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.2108 - acc: 0.9473 - val_loss: 0.3607 - val_acc: 0.8786\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.2052 - acc: 0.9483 - val_loss: 0.3752 - val_acc: 0.8750\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1922 - acc: 0.9530 - val_loss: 0.3950 - val_acc: 0.8699\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1998 - acc: 0.9490 - val_loss: 0.3976 - val_acc: 0.8695\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.1901 - acc: 0.9552 - val_loss: 0.4378 - val_acc: 0.8578\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.1866 - acc: 0.9548 - val_loss: 0.4015 - val_acc: 0.8702\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 42ms/step - loss: 0.1864 - acc: 0.9555 - val_loss: 0.3950 - val_acc: 0.8722\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1844 - acc: 0.9557 - val_loss: 0.4009 - val_acc: 0.8717\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.1818 - acc: 0.9574 - val_loss: 0.4599 - val_acc: 0.8591\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1849 - acc: 0.9543 - val_loss: 0.4319 - val_acc: 0.8652\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1780 - acc: 0.9589 - val_loss: 0.4182 - val_acc: 0.8678\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.1801 - acc: 0.9565 - val_loss: 0.4809 - val_acc: 0.8514\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1722 - acc: 0.9595 - val_loss: 0.4384 - val_acc: 0.8661\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1703 - acc: 0.9599 - val_loss: 0.4301 - val_acc: 0.8662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U56FLlewmpRe"
      },
      "source": [
        "#Exe. 14 Plot the validation loss w.r.t epochs changing for original model and l2_model.\n",
        "\n",
        "Observe, the model with L2 regularization and see how it is more\n",
        "resistant to overfitting than the original model, even though both models have\n",
        "the same number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kVXOa43Bmv0m",
        "outputId": "0ee767a6-32f5-4c23-b9f9-7592632b86c0"
      },
      "source": [
        "l2_model_loss=l2_model_hist.history['val_loss']\n",
        "epochs = range(1,21)\n",
        "plt.plot(epochs, l2_model_loss, 'b', linestyle='', marker='+',label='L2 model')\n",
        "plt.plot(epochs, original_loss, 'b', linestyle='', marker='o', label='Original model')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxjpaaa0nn17"
      },
      "source": [
        "We can observe that using L2_model, the loss is much lesser than that of original model, although we used same model parameters(number of nodes and number of layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9wUtrARoiLe"
      },
      "source": [
        "# Adding Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooPOuDzMokip"
      },
      "source": [
        "Dropout, applied to a layer, consists of randomly “dropping out” (i.e. setting to\n",
        "zero) a number of output features of the layer during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m0zxQUApxFt"
      },
      "source": [
        "The dropout can be added to the model in Keras using **layers.Dropout(0.5)**\n",
        "where 0.5 is the dropout rate indicating 50% of nodes should be dropped for\n",
        "the following layer randomly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2rkCqBSqymx"
      },
      "source": [
        "The dropout should be applied to the output of layer right before the layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORUY_St8q3oU"
      },
      "source": [
        "#Exe. 15 Modify the original network by adding a dropout after each layer of 16 nodes. Name this model dpt_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b2k6NvEEq-Pj"
      },
      "source": [
        "dpt_model = keras.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "dpt_model.compile(optimizer ='rmsprop',loss ='binary_crossentropy',metrics =['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiYtw6xbsW61"
      },
      "source": [
        "#Exe. 16 Fit the model on the set with previously presented parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hg7WXz0csVsl",
        "outputId": "9a31b5f9-e307-4434-bd90-f912632da50e"
      },
      "source": [
        "dpt_model_hist = dpt_model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs =20,batch_size = 512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 3s 48ms/step - loss: 0.6511 - acc: 0.6032 - val_loss: 0.4674 - val_acc: 0.8662\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.4879 - acc: 0.7827 - val_loss: 0.3524 - val_acc: 0.8814\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.3956 - acc: 0.8476 - val_loss: 0.3002 - val_acc: 0.8867\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.3231 - acc: 0.8866 - val_loss: 0.2865 - val_acc: 0.8852\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.2708 - acc: 0.9100 - val_loss: 0.2760 - val_acc: 0.8894\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.2328 - acc: 0.9240 - val_loss: 0.2849 - val_acc: 0.8888\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.2150 - acc: 0.9313 - val_loss: 0.3036 - val_acc: 0.8874\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 2s 41ms/step - loss: 0.1862 - acc: 0.9389 - val_loss: 0.3160 - val_acc: 0.8853\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1729 - acc: 0.9433 - val_loss: 0.3481 - val_acc: 0.8788\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1601 - acc: 0.9467 - val_loss: 0.3610 - val_acc: 0.8814\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.1536 - acc: 0.9511 - val_loss: 0.3898 - val_acc: 0.8738\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 2s 37ms/step - loss: 0.1440 - acc: 0.9525 - val_loss: 0.3958 - val_acc: 0.8783\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1316 - acc: 0.9599 - val_loss: 0.4244 - val_acc: 0.8776\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 2s 39ms/step - loss: 0.1285 - acc: 0.9577 - val_loss: 0.4496 - val_acc: 0.8762\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1187 - acc: 0.9642 - val_loss: 0.4610 - val_acc: 0.8741\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1218 - acc: 0.9588 - val_loss: 0.4845 - val_acc: 0.8747\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1161 - acc: 0.9636 - val_loss: 0.4950 - val_acc: 0.8721\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 2s 40ms/step - loss: 0.1074 - acc: 0.9665 - val_loss: 0.5228 - val_acc: 0.8689\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.1033 - acc: 0.9696 - val_loss: 0.5518 - val_acc: 0.8694\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 2s 38ms/step - loss: 0.1003 - acc: 0.9685 - val_loss: 0.5863 - val_acc: 0.8699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeIUTU4osp7n"
      },
      "source": [
        "#Exe. 17 Plot the validation loss changing w.r.t the epochs changing for two original and dropout model. \n",
        "What are your observations? Do you see any improvement over the original network?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "49LrTiIityMN",
        "outputId": "8ae1d01a-649e-4409-93b9-e5401333bc60"
      },
      "source": [
        "dpt_model_loss=dpt_model_hist.history['val_loss']\n",
        "epochs = range(1,21)\n",
        "plt.plot(epochs, dpt_model_loss, 'b', linestyle='', marker='+',label='DropOut model')\n",
        "plt.plot(epochs, original_loss, 'b', linestyle='', marker='o', label='Original model')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvdgok-ruHHA"
      },
      "source": [
        "We can observe that as compared to the original model, drop out model has lesser validation loss even though the model parameters (no of nodes and no of layers) are same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLkixv4zwhPf"
      },
      "source": [
        "#House Pricing DataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7b0VMrIwn3b"
      },
      "source": [
        "#Exe. 1 Load House Price DataSet\n",
        "The goal is to predict the median price of homes in a given Boston suburb in the mid-1970s, given data points\n",
        "about the suburb at the time, such as the crime rate, the local property tax\n",
        "rate, and so on. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqSPhUdiwzGA"
      },
      "source": [
        "Load the boston_housing dataset in train and test and check\n",
        "the data shape and sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CVxCc2ycw21d"
      },
      "source": [
        "try :\n",
        "# % tensorflow_version only exists in Colab .\n",
        "  % tensorflow_version 2.x\n",
        "except Exception :\n",
        "  pass\n",
        "from keras.datasets import boston_housing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "odUlflsbxIMc",
        "outputId": "5622ba60-f9f5-4937-dc53-9cbce2bc0492"
      },
      "source": [
        "( train_data , train_targets ) , ( test_data , test_targets ) = boston_housing.load_data()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tgHwqEvJziBZ",
        "outputId": "63fe0d86-4cdf-4b00-ee25-eef9f824fa20"
      },
      "source": [
        "print(\"training data shape:- \",train_data.shape)\n",
        "print(\"testing data shape:- \",test_data.shape)\n",
        "print(\"training data dimensions:- \",train_data.ndim)\n",
        "print(\"testing data dimensions:- \",test_data.ndim)\n",
        "print(\"----------------------------------------------\")\n",
        "print(\"train_target shape:- \",train_targets.shape)\n",
        "print(\"test_target shape:- \",test_targets.shape)\n",
        "print(\"train_target dimensions:- \",train_targets.ndim)\n",
        "print(\"test_target dimensions:- \",test_targets.ndim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data shape:-  (404, 13)\n",
            "testing data shape:-  (102, 13)\n",
            "training data dimensions:-  2\n",
            "testing data dimensions:-  2\n",
            "----------------------------------------------\n",
            "train_target shape:-  (404,)\n",
            "test_target shape:-  (102,)\n",
            "train_target dimensions:-  1\n",
            "test_target dimensions:-  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B54__rGU01iJ",
        "outputId": "62084579-e227-464b-c1a2-2e2af7aed135"
      },
      "source": [
        "print(\"Train data:-\\n\\n\",train_data)\n",
        "print(\"---------------------------------------------------------------------------\\n\")\n",
        "print(\"Test data:-\\n\\n\",test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data:-\n",
            "\n",
            " [[1.23247e+00 0.00000e+00 8.14000e+00 ... 2.10000e+01 3.96900e+02\n",
            "  1.87200e+01]\n",
            " [2.17700e-02 8.25000e+01 2.03000e+00 ... 1.47000e+01 3.95380e+02\n",
            "  3.11000e+00]\n",
            " [4.89822e+00 0.00000e+00 1.81000e+01 ... 2.02000e+01 3.75520e+02\n",
            "  3.26000e+00]\n",
            " ...\n",
            " [3.46600e-02 3.50000e+01 6.06000e+00 ... 1.69000e+01 3.62250e+02\n",
            "  7.83000e+00]\n",
            " [2.14918e+00 0.00000e+00 1.95800e+01 ... 1.47000e+01 2.61950e+02\n",
            "  1.57900e+01]\n",
            " [1.43900e-02 6.00000e+01 2.93000e+00 ... 1.56000e+01 3.76700e+02\n",
            "  4.38000e+00]]\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "Test data:-\n",
            "\n",
            " [[1.80846e+01 0.00000e+00 1.81000e+01 ... 2.02000e+01 2.72500e+01\n",
            "  2.90500e+01]\n",
            " [1.23290e-01 0.00000e+00 1.00100e+01 ... 1.78000e+01 3.94950e+02\n",
            "  1.62100e+01]\n",
            " [5.49700e-02 0.00000e+00 5.19000e+00 ... 2.02000e+01 3.96900e+02\n",
            "  9.74000e+00]\n",
            " ...\n",
            " [1.83377e+00 0.00000e+00 1.95800e+01 ... 1.47000e+01 3.89610e+02\n",
            "  1.92000e+00]\n",
            " [3.58090e-01 0.00000e+00 6.20000e+00 ... 1.74000e+01 3.91700e+02\n",
            "  9.71000e+00]\n",
            " [2.92400e+00 0.00000e+00 1.95800e+01 ... 1.47000e+01 2.40160e+02\n",
            "  9.81000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgTGkYR_1L_5"
      },
      "source": [
        "The targets are the median values of owner-occupied homes, in thousands\n",
        "of dollars. Print them to check their data 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i5LbChfj1ORm",
        "outputId": "49d24485-daaa-40bf-d1d6-a9a19fae8843"
      },
      "source": [
        "print(\"Train targets:-\\n\\n\",train_targets)\n",
        "print(\"-------------------------------------------------------------------------------\\n\")\n",
        "print(\"Test targets:-\\n\\n\",test_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train targets:-\n",
            "\n",
            " [15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4 12.1 17.9 23.1 19.9\n",
            " 15.7  8.8 50.  22.5 24.1 27.5 10.9 30.8 32.9 24.  18.5 13.3 22.9 34.7\n",
            " 16.6 17.5 22.3 16.1 14.9 23.1 34.9 25.  13.9 13.1 20.4 20.  15.2 24.7\n",
            " 22.2 16.7 12.7 15.6 18.4 21.  30.1 15.1 18.7  9.6 31.5 24.8 19.1 22.\n",
            " 14.5 11.  32.  29.4 20.3 24.4 14.6 19.5 14.1 14.3 15.6 10.5  6.3 19.3\n",
            " 19.3 13.4 36.4 17.8 13.5 16.5  8.3 14.3 16.  13.4 28.6 43.5 20.2 22.\n",
            " 23.  20.7 12.5 48.5 14.6 13.4 23.7 50.  21.7 39.8 38.7 22.2 34.9 22.5\n",
            " 31.1 28.7 46.  41.7 21.  26.6 15.  24.4 13.3 21.2 11.7 21.7 19.4 50.\n",
            " 22.8 19.7 24.7 36.2 14.2 18.9 18.3 20.6 24.6 18.2  8.7 44.  10.4 13.2\n",
            " 21.2 37.  30.7 22.9 20.  19.3 31.7 32.  23.1 18.8 10.9 50.  19.6  5.\n",
            " 14.4 19.8 13.8 19.6 23.9 24.5 25.  19.9 17.2 24.6 13.5 26.6 21.4 11.9\n",
            " 22.6 19.6  8.5 23.7 23.1 22.4 20.5 23.6 18.4 35.2 23.1 27.9 20.6 23.7\n",
            " 28.  13.6 27.1 23.6 20.6 18.2 21.7 17.1  8.4 25.3 13.8 22.2 18.4 20.7\n",
            " 31.6 30.5 20.3  8.8 19.2 19.4 23.1 23.  14.8 48.8 22.6 33.4 21.1 13.6\n",
            " 32.2 13.1 23.4 18.9 23.9 11.8 23.3 22.8 19.6 16.7 13.4 22.2 20.4 21.8\n",
            " 26.4 14.9 24.1 23.8 12.3 29.1 21.  19.5 23.3 23.8 17.8 11.5 21.7 19.9\n",
            " 25.  33.4 28.5 21.4 24.3 27.5 33.1 16.2 23.3 48.3 22.9 22.8 13.1 12.7\n",
            " 22.6 15.  15.3 10.5 24.  18.5 21.7 19.5 33.2 23.2  5.  19.1 12.7 22.3\n",
            " 10.2 13.9 16.3 17.  20.1 29.9 17.2 37.3 45.4 17.8 23.2 29.  22.  18.\n",
            " 17.4 34.6 20.1 25.  15.6 24.8 28.2 21.2 21.4 23.8 31.  26.2 17.4 37.9\n",
            " 17.5 20.   8.3 23.9  8.4 13.8  7.2 11.7 17.1 21.6 50.  16.1 20.4 20.6\n",
            " 21.4 20.6 36.5  8.5 24.8 10.8 21.9 17.3 18.9 36.2 14.9 18.2 33.3 21.8\n",
            " 19.7 31.6 24.8 19.4 22.8  7.5 44.8 16.8 18.7 50.  50.  19.5 20.1 50.\n",
            " 17.2 20.8 19.3 41.3 20.4 20.5 13.8 16.5 23.9 20.6 31.5 23.3 16.8 14.\n",
            " 33.8 36.1 12.8 18.3 18.7 19.1 29.  30.1 50.  50.  22.  11.9 37.6 50.\n",
            " 22.7 20.8 23.5 27.9 50.  19.3 23.9 22.6 15.2 21.7 19.2 43.8 20.3 33.2\n",
            " 19.9 22.5 32.7 22.  17.1 19.  15.  16.1 25.1 23.7 28.7 37.2 22.6 16.4\n",
            " 25.  29.8 22.1 17.4 18.1 30.3 17.5 24.7 12.6 26.5 28.7 13.3 10.4 24.4\n",
            " 23.  20.  17.8  7.  11.8 24.4 13.8 19.4 25.2 19.4 19.4 29.1]\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            "Test targets:-\n",
            "\n",
            " [ 7.2 18.8 19.  27.  22.2 24.5 31.2 22.9 20.5 23.2 18.6 14.5 17.8 50.\n",
            " 20.8 24.3 24.2 19.8 19.1 22.7 12.  10.2 20.  18.5 20.9 23.  27.5 30.1\n",
            "  9.5 22.  21.2 14.1 33.1 23.4 20.1  7.4 15.4 23.8 20.1 24.5 33.  28.4\n",
            " 14.1 46.7 32.5 29.6 28.4 19.8 20.2 25.  35.4 20.3  9.7 14.5 34.9 26.6\n",
            "  7.2 50.  32.4 21.6 29.8 13.1 27.5 21.2 23.1 21.9 13.  23.2  8.1  5.6\n",
            " 21.7 29.6 19.6  7.  26.4 18.9 20.9 28.1 35.4 10.2 24.3 43.1 17.6 15.4\n",
            " 16.2 27.1 21.4 21.5 22.4 25.  16.6 18.6 22.  42.8 35.1 21.5 36.  21.9\n",
            " 24.1 50.  26.7 25. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmB8PwSl2IRm"
      },
      "source": [
        "#Exe. 2 Normalizing data For each feature in the input data\n",
        "(a column in the input data matrix), subtract the mean of the feature and divide by the standard deviation i.e. **∀x, x = x−µ/σ**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFZ7osQ52h_u"
      },
      "source": [
        "In this way the feature is centered around 0 and\n",
        "has a unit standard deviation. \n",
        "**Hint**: use **.mean(axis=0)** and **.std(axis=0)** for computing mean and standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fq4EXYML27e1",
        "outputId": "d10bc174-c475-446c-f522-d708034b07cb"
      },
      "source": [
        "\"\"\"\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "def normalize_data(data):\n",
        "  # create scaler\n",
        "  scaler = StandardScaler()\n",
        "  # fit scaler on data\n",
        "  scaler.fit(data)\n",
        "  # apply transform\n",
        "  standardized = scaler.transform(data)\n",
        "  return standardized\n",
        "normalize_data(train_data)\n",
        "normalize_data(test_data)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom sklearn.preprocessing import StandardScaler\\ndef normalize_data(data):\\n  # create scaler\\n  scaler = StandardScaler()\\n  # fit scaler on data\\n  scaler.fit(data)\\n  # apply transform\\n  standardized = scaler.transform(data)\\n  return standardized\\nnormalize_data(train_data)\\nnormalize_data(test_data)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qoZZDMcQ4g2L"
      },
      "source": [
        "def normalize_data(data): \n",
        "  mean = data.mean(axis = 0)\n",
        "  std = data.std(axis = 0)\n",
        "  for i in range(len(data)): \n",
        "    row = data[i]\n",
        "    for j in range(len(row)): \n",
        "      if(std[j]!=0):\n",
        "        data[i][j] = (row[j] - mean[j])/(std[j])\n",
        "      else:\n",
        "        data[i][j] = (row[j] - mean[j])/1\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZMz5pxfi8JEt",
        "outputId": "35242e9a-8328-4679-965c-a67149b85c92"
      },
      "source": [
        "train_data=normalize_data(train_data)\n",
        "test_data=normalize_data(test_data)\n",
        "print(\"Normalized train data:-\\n\\n\",train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized train data:-\n",
            "\n",
            " [[-0.27224633 -0.48361547 -0.43576161 ...  1.14850044  0.44807713\n",
            "   0.8252202 ]\n",
            " [-0.40342651  2.99178419 -1.33391162 ... -1.71818909  0.43190599\n",
            "  -1.32920239]\n",
            " [ 0.1249402  -0.48361547  1.0283258  ...  0.78447637  0.22061726\n",
            "  -1.30850006]\n",
            " ...\n",
            " [-0.40202987  0.99079651 -0.7415148  ... -0.71712291  0.07943894\n",
            "  -0.67776904]\n",
            " [-0.17292018 -0.48361547  1.24588095 ... -1.71818909 -0.98764362\n",
            "   0.42083466]\n",
            " [-0.40422614  2.04394792 -1.20161456 ... -1.30866202  0.23317118\n",
            "  -1.15392266]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v_hEg77B993f",
        "outputId": "315fa975-d647-4d6b-e792-499bab0ca2d1"
      },
      "source": [
        "print(\"Normalized test data:-\\n\\n\",test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized test data:-\n",
            "\n",
            " [[ 2.8040301  -0.50784934  0.96960877 ...  0.90513041 -4.27829517\n",
            "   2.51324773]\n",
            " [-0.55530596 -0.50784934 -0.17801704 ... -0.28485844  0.3909446\n",
            "   0.58604286]\n",
            " [-0.56808398 -0.50784934 -0.86176938 ...  0.90513041  0.41570668\n",
            "  -0.38506427]\n",
            " ...\n",
            " [-0.23539182 -0.50784934  1.17955762 ... -1.82192738  0.32313459\n",
            "  -1.55879807]\n",
            " [-0.5113909  -0.50784934 -0.71849348 ... -0.48318992  0.34967446\n",
            "  -0.38956708]\n",
            " [-0.03148414 -0.50784934  1.17955762 ... -1.82192738 -1.57465677\n",
            "  -0.3745577 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA2OpfT3_fNu"
      },
      "source": [
        "#Exe. 3 Model definition\n",
        "Because so few samples are available, use a very\n",
        "small network with **two hidden layers**, each with **64** units and **relu** activation.\n",
        "To have a linear layer output, add a **final layer with a single unit** and no activation function. \n",
        "\n",
        "Compile the network with the **mse** ( mean squared error)\n",
        "loss function. Monitoring a new metric during training: mean absolute error (\n",
        "**mae** )\n",
        "Finally define the optimizer = **'rmsprop'** in the model compilation.\n",
        "\n",
        "Define the model in a function named **build_model**()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qXy1Sr8m_y77"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  # complete the model here\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation = None))\n",
        "  # compile model\n",
        "  model.compile(optimizer ='rmsprop',loss='mean_squared_error',metrics =['mae'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWM8-vh2C-kT"
      },
      "source": [
        "#Exe. 4 Model Validation\n",
        "\n",
        "Using K-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xsm_0MemAyiU",
        "outputId": "ce7f26ea-eb7e-4428-ff93-e52c6084d8ce"
      },
      "source": [
        "import numpy as np\n",
        "# using k=4 iterations\n",
        "k = 4\n",
        "num_val_samples = len( train_data ) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "for i in range (k ):\n",
        "  print ('processing fold #', i+1,\"/\",k )\n",
        "  val_data = train_data [i * num_val_samples : ( i + 1) *num_val_samples ]\n",
        "  val_targets = train_targets [i * num_val_samples : (i + 1) *num_val_samples ]\n",
        "  partial_train_data = np.concatenate ([ train_data [: i * num_val_samples ], train_data [( i + 1) * num_val_samples :]] , axis=0)\n",
        "  partial_train_targets = np . concatenate ([ train_targets [: i *num_val_samples ], train_targets [( i + 1) * num_val_samples :]] ,axis =0)\n",
        "  # you should call build_model here\n",
        "  model=build_model()\n",
        "  # you should fit model here\n",
        "  model_hist=model.fit(partial_train_data, partial_train_targets,validation_data=(val_data,val_targets), epochs = num_epochs)\n",
        "  #the following line is for evaluating the test set\n",
        "  val_mse , val_mae = model.evaluate(val_data, val_targets , verbose=0)\n",
        "  all_scores.append(val_mae )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 1 / 4\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 25ms/step - loss: 555.0561 - mae: 21.9072 - val_loss: 471.7053 - val_mae: 19.6249\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 482.7290 - mae: 20.3059 - val_loss: 391.0776 - val_mae: 17.6710\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 377.8820 - mae: 17.7245 - val_loss: 303.7855 - val_mae: 15.3467\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 309.8066 - mae: 15.8760 - val_loss: 223.1135 - val_mae: 12.8371\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 222.6004 - mae: 12.9384 - val_loss: 153.6267 - val_mae: 10.2334\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 155.1755 - mae: 10.4481 - val_loss: 99.8213 - val_mae: 7.7659\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 113.1674 - mae: 8.2888 - val_loss: 65.6357 - val_mae: 5.9384\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 84.5510 - mae: 7.0726 - val_loss: 50.0102 - val_mae: 5.0077\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 51.7607 - mae: 5.7061 - val_loss: 40.4041 - val_mae: 4.3834\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 43.3669 - mae: 5.0819 - val_loss: 34.3910 - val_mae: 4.0164\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 39.0969 - mae: 4.6634 - val_loss: 30.4189 - val_mae: 3.6790\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 37.3148 - mae: 4.3447 - val_loss: 27.0851 - val_mae: 3.3815\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 30.7516 - mae: 4.0617 - val_loss: 25.2092 - val_mae: 3.3044\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 30.3274 - mae: 3.9882 - val_loss: 24.1901 - val_mae: 3.2476\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 29.3928 - mae: 3.7757 - val_loss: 22.4666 - val_mae: 3.0623\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 24.1360 - mae: 3.6307 - val_loss: 21.1225 - val_mae: 2.9917\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 21.2520 - mae: 3.3435 - val_loss: 20.3386 - val_mae: 3.0041\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 19.2954 - mae: 3.2432 - val_loss: 19.0772 - val_mae: 2.8513\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 22.0547 - mae: 3.4127 - val_loss: 18.4633 - val_mae: 2.8392\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.0090 - mae: 3.0642 - val_loss: 17.7299 - val_mae: 2.8468\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.2257 - mae: 3.0145 - val_loss: 16.8308 - val_mae: 2.7895\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 21.2393 - mae: 3.2511 - val_loss: 15.7655 - val_mae: 2.6271\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 16.7183 - mae: 2.9622 - val_loss: 15.2534 - val_mae: 2.6224\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.2472 - mae: 2.8689 - val_loss: 14.8567 - val_mae: 2.6229\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 14.1465 - mae: 2.6887 - val_loss: 13.5124 - val_mae: 2.4798\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 15.3175 - mae: 2.7969 - val_loss: 13.5794 - val_mae: 2.5452\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 12.4656 - mae: 2.5724 - val_loss: 12.4590 - val_mae: 2.4515\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 12.6732 - mae: 2.5534 - val_loss: 12.5286 - val_mae: 2.4258\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 11.0925 - mae: 2.4802 - val_loss: 11.6333 - val_mae: 2.2999\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 12.0028 - mae: 2.4160 - val_loss: 11.3202 - val_mae: 2.2659\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 12.0351 - mae: 2.4599 - val_loss: 10.9754 - val_mae: 2.2638\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 14.0486 - mae: 2.5623 - val_loss: 11.0676 - val_mae: 2.3048\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 13.2938 - mae: 2.4516 - val_loss: 10.2850 - val_mae: 2.2111\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.3931 - mae: 2.4002 - val_loss: 10.4293 - val_mae: 2.2736\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.5362 - mae: 2.4705 - val_loss: 9.8613 - val_mae: 2.1938\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.7899 - mae: 2.2140 - val_loss: 9.2793 - val_mae: 2.0984\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 12.7554 - mae: 2.4823 - val_loss: 9.7097 - val_mae: 2.2693\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.8378 - mae: 2.2073 - val_loss: 9.1586 - val_mae: 2.1458\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.6634 - mae: 2.3883 - val_loss: 8.8379 - val_mae: 2.1193\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.2894 - mae: 2.4555 - val_loss: 8.7455 - val_mae: 2.0336\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.6709 - mae: 2.2716 - val_loss: 8.6296 - val_mae: 2.0176\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.7608 - mae: 2.3109 - val_loss: 8.4705 - val_mae: 1.9865\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.7784 - mae: 2.1585 - val_loss: 8.9328 - val_mae: 2.2026\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 12.5405 - mae: 2.3573 - val_loss: 8.7785 - val_mae: 2.0762\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.2878 - mae: 2.2616 - val_loss: 8.1180 - val_mae: 1.9656\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.7283 - mae: 2.1797 - val_loss: 8.2689 - val_mae: 1.9420\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.6754 - mae: 2.2360 - val_loss: 9.2516 - val_mae: 2.3143\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 9.3953 - mae: 2.2480 - val_loss: 8.2598 - val_mae: 2.0405\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.7280 - mae: 2.1258 - val_loss: 7.9882 - val_mae: 2.0220\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.5654 - mae: 2.1122 - val_loss: 8.1985 - val_mae: 2.1273\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.4800 - mae: 2.1730 - val_loss: 8.2440 - val_mae: 2.0586\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.2802 - mae: 2.2624 - val_loss: 8.7180 - val_mae: 2.0979\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 8.0015 - mae: 2.0195 - val_loss: 7.4499 - val_mae: 1.8789\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.3870 - mae: 2.0978 - val_loss: 7.4054 - val_mae: 1.8698\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 9.2305 - mae: 2.1097 - val_loss: 7.3719 - val_mae: 1.8771\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.8350 - mae: 2.0141 - val_loss: 7.7056 - val_mae: 1.9972\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.1405 - mae: 2.1027 - val_loss: 7.7679 - val_mae: 2.0272\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.2156 - mae: 2.0626 - val_loss: 7.2797 - val_mae: 1.8594\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.1854 - mae: 2.0698 - val_loss: 7.3198 - val_mae: 1.8804\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.1617 - mae: 1.9881 - val_loss: 7.7373 - val_mae: 1.9400\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 9.3312 - mae: 2.0662 - val_loss: 8.1764 - val_mae: 2.0079\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.6178 - mae: 2.0927 - val_loss: 7.6166 - val_mae: 1.8526\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.8471 - mae: 2.0997 - val_loss: 7.7936 - val_mae: 1.9497\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.1284 - mae: 1.9813 - val_loss: 8.0210 - val_mae: 2.0472\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.6682 - mae: 2.0432 - val_loss: 7.2384 - val_mae: 1.8276\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.9850 - mae: 2.0736 - val_loss: 7.8831 - val_mae: 1.9416\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.2271 - mae: 1.8866 - val_loss: 8.1129 - val_mae: 2.0733\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7221 - mae: 1.9221 - val_loss: 7.1694 - val_mae: 1.8010\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.9198 - mae: 2.1634 - val_loss: 7.1525 - val_mae: 1.7916\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 9.9370 - mae: 2.1889 - val_loss: 7.3370 - val_mae: 1.9435\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.1689 - mae: 2.0387 - val_loss: 7.5467 - val_mae: 1.9159\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.5606 - mae: 1.9548 - val_loss: 7.3312 - val_mae: 1.9031\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.9744 - mae: 1.8858 - val_loss: 7.1909 - val_mae: 1.9418\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.2771 - mae: 1.9850 - val_loss: 6.9474 - val_mae: 1.8451\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.4110 - mae: 1.8879 - val_loss: 7.3148 - val_mae: 1.9791\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.4428 - mae: 2.0659 - val_loss: 6.8296 - val_mae: 1.8225\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.9199 - mae: 1.8077 - val_loss: 7.4528 - val_mae: 1.9757\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 9.7835 - mae: 2.0217 - val_loss: 6.8579 - val_mae: 1.8794\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.8349 - mae: 1.9400 - val_loss: 7.3940 - val_mae: 1.9362\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.9104 - mae: 1.9687 - val_loss: 6.7564 - val_mae: 1.8380\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.4173 - mae: 1.8996 - val_loss: 6.8069 - val_mae: 1.8173\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.2714 - mae: 2.0997 - val_loss: 7.4273 - val_mae: 1.9882\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.6681 - mae: 2.0236 - val_loss: 8.1090 - val_mae: 2.0429\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.9473 - mae: 1.9725 - val_loss: 6.6883 - val_mae: 1.7972\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.1526 - mae: 2.0711 - val_loss: 6.6633 - val_mae: 1.8025\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.7253 - mae: 1.9583 - val_loss: 6.9607 - val_mae: 1.9416\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.4418 - mae: 1.9195 - val_loss: 7.2144 - val_mae: 1.8953\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.5180 - mae: 1.9434 - val_loss: 7.0944 - val_mae: 1.9671\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.4718 - mae: 1.8512 - val_loss: 6.6520 - val_mae: 1.7970\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.0760 - mae: 1.9319 - val_loss: 7.3087 - val_mae: 1.9217\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.2065 - mae: 1.9511 - val_loss: 6.8370 - val_mae: 1.8721\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.4293 - mae: 1.8611 - val_loss: 6.6670 - val_mae: 1.8582\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.3608 - mae: 1.8872 - val_loss: 7.2888 - val_mae: 2.0198\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.3676 - mae: 1.9659 - val_loss: 7.2452 - val_mae: 1.9160\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.9911 - mae: 1.8339 - val_loss: 6.6824 - val_mae: 1.8564\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.1774 - mae: 1.9058 - val_loss: 6.9775 - val_mae: 1.8944\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.7730 - mae: 1.7830 - val_loss: 7.2101 - val_mae: 2.0583\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.1549 - mae: 1.9339 - val_loss: 6.7905 - val_mae: 1.8672\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.1516 - mae: 1.7868 - val_loss: 6.6198 - val_mae: 1.8500\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.9817 - mae: 1.8988 - val_loss: 7.0636 - val_mae: 1.8804\n",
            "processing fold # 2 / 4\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 20ms/step - loss: 572.0269 - mae: 21.8031 - val_loss: 472.7130 - val_mae: 19.9155\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 547.6679 - mae: 20.8372 - val_loss: 407.9125 - val_mae: 18.2421\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 407.2003 - mae: 17.9858 - val_loss: 333.9460 - val_mae: 16.2517\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 330.7802 - mae: 15.7603 - val_loss: 257.7050 - val_mae: 14.0366\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 259.8716 - mae: 13.6432 - val_loss: 184.8943 - val_mae: 11.6666\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 180.2392 - mae: 11.1424 - val_loss: 125.5711 - val_mae: 9.4286\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 111.1748 - mae: 8.4615 - val_loss: 81.3833 - val_mae: 7.4528\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 85.2991 - mae: 7.4104 - val_loss: 55.0373 - val_mae: 6.0183\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 57.6299 - mae: 5.9601 - val_loss: 40.3633 - val_mae: 5.0546\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 41.4955 - mae: 5.0721 - val_loss: 31.5152 - val_mae: 4.4282\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 35.9721 - mae: 4.5956 - val_loss: 26.2318 - val_mae: 3.9844\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 31.8214 - mae: 4.2853 - val_loss: 23.2113 - val_mae: 3.7636\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 32.0524 - mae: 3.9289 - val_loss: 22.1750 - val_mae: 3.6627\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 30.3487 - mae: 3.7928 - val_loss: 20.9815 - val_mae: 3.5744\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 22.3483 - mae: 3.4623 - val_loss: 18.9591 - val_mae: 3.3344\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 23.9259 - mae: 3.3664 - val_loss: 18.6675 - val_mae: 3.3649\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 23.5283 - mae: 3.3827 - val_loss: 19.5572 - val_mae: 3.5038\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 19.8167 - mae: 3.1635 - val_loss: 17.4089 - val_mae: 3.2512\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 15.7261 - mae: 2.8394 - val_loss: 16.9513 - val_mae: 3.1775\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 14.2594 - mae: 2.7133 - val_loss: 16.1265 - val_mae: 3.0963\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 16.8735 - mae: 2.8854 - val_loss: 15.6428 - val_mae: 3.0371\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 18.9399 - mae: 2.9705 - val_loss: 14.8214 - val_mae: 2.9050\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 11.7996 - mae: 2.4586 - val_loss: 15.2920 - val_mae: 2.9970\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 17.0934 - mae: 2.7606 - val_loss: 14.5249 - val_mae: 2.9282\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 15.2128 - mae: 2.6904 - val_loss: 13.9595 - val_mae: 2.8572\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 11.9833 - mae: 2.4271 - val_loss: 13.9554 - val_mae: 2.8842\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 13.7397 - mae: 2.5260 - val_loss: 13.2598 - val_mae: 2.7883\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.6246 - mae: 2.4355 - val_loss: 13.8929 - val_mae: 2.9157\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 14.2907 - mae: 2.5332 - val_loss: 14.2241 - val_mae: 2.9531\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.6038 - mae: 2.3394 - val_loss: 12.2281 - val_mae: 2.6510\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.0974 - mae: 2.3169 - val_loss: 12.2650 - val_mae: 2.6922\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.8947 - mae: 2.4618 - val_loss: 13.1166 - val_mae: 2.8126\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 11.4012 - mae: 2.3998 - val_loss: 12.3639 - val_mae: 2.6683\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 12.1387 - mae: 2.3144 - val_loss: 11.6917 - val_mae: 2.5947\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.9152 - mae: 2.2380 - val_loss: 11.9880 - val_mae: 2.6656\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 11.8083 - mae: 2.3000 - val_loss: 11.6848 - val_mae: 2.6350\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.9340 - mae: 2.2138 - val_loss: 11.5567 - val_mae: 2.5833\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.8794 - mae: 2.1315 - val_loss: 11.8127 - val_mae: 2.6295\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 11.9964 - mae: 2.2906 - val_loss: 11.5083 - val_mae: 2.6039\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 11.7991 - mae: 2.4070 - val_loss: 10.9242 - val_mae: 2.5135\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.4468 - mae: 2.1285 - val_loss: 11.0887 - val_mae: 2.5635\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 8.8347 - mae: 2.0226 - val_loss: 12.3210 - val_mae: 2.7094\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.7560 - mae: 2.1304 - val_loss: 11.5951 - val_mae: 2.6415\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.5194 - mae: 2.1715 - val_loss: 10.8879 - val_mae: 2.5239\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 9.1701 - mae: 2.1404 - val_loss: 11.6597 - val_mae: 2.6311\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.4296 - mae: 2.0404 - val_loss: 13.0174 - val_mae: 2.8469\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.6414 - mae: 2.0885 - val_loss: 10.6244 - val_mae: 2.4787\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 9.7392 - mae: 2.0515 - val_loss: 10.6804 - val_mae: 2.5030\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.1358 - mae: 2.0279 - val_loss: 11.1627 - val_mae: 2.5879\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.7699 - mae: 2.1229 - val_loss: 11.4605 - val_mae: 2.6316\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.9154 - mae: 2.1608 - val_loss: 10.4031 - val_mae: 2.4751\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.9548 - mae: 1.9088 - val_loss: 10.7117 - val_mae: 2.5168\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.9963 - mae: 2.1151 - val_loss: 10.7444 - val_mae: 2.5192\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.9051 - mae: 1.9679 - val_loss: 11.9413 - val_mae: 2.7222\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.8706 - mae: 1.9313 - val_loss: 10.3552 - val_mae: 2.4844\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.2901 - mae: 2.0791 - val_loss: 10.6206 - val_mae: 2.5137\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.1000 - mae: 2.0076 - val_loss: 11.3372 - val_mae: 2.6196\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.9369 - mae: 2.0378 - val_loss: 11.2502 - val_mae: 2.6213\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.6348 - mae: 2.0137 - val_loss: 10.5905 - val_mae: 2.5228\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.2745 - mae: 2.0149 - val_loss: 10.1115 - val_mae: 2.4457\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.8085 - mae: 2.0701 - val_loss: 10.3776 - val_mae: 2.4840\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 12.1600 - mae: 2.3096 - val_loss: 10.3523 - val_mae: 2.4718\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.6386 - mae: 2.0068 - val_loss: 10.1240 - val_mae: 2.4454\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.6838 - mae: 2.0221 - val_loss: 10.4774 - val_mae: 2.5181\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.3102 - mae: 1.9224 - val_loss: 10.5649 - val_mae: 2.5313\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.4744 - mae: 2.0133 - val_loss: 11.1089 - val_mae: 2.6205\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.7187 - mae: 1.9639 - val_loss: 11.6618 - val_mae: 2.7114\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.2188 - mae: 1.9010 - val_loss: 11.4065 - val_mae: 2.6648\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.6933 - mae: 1.8256 - val_loss: 10.2478 - val_mae: 2.4880\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.9285 - mae: 1.8541 - val_loss: 10.8990 - val_mae: 2.5956\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.2739 - mae: 1.9258 - val_loss: 10.6603 - val_mae: 2.5653\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.3658 - mae: 1.9266 - val_loss: 10.7829 - val_mae: 2.5774\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.4975 - mae: 1.8274 - val_loss: 11.8795 - val_mae: 2.7441\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.8654 - mae: 2.0045 - val_loss: 10.0690 - val_mae: 2.4654\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.1643 - mae: 1.9381 - val_loss: 10.0165 - val_mae: 2.4574\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.4079 - mae: 1.9524 - val_loss: 9.6468 - val_mae: 2.3941\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.5678 - mae: 1.9824 - val_loss: 10.8003 - val_mae: 2.5823\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.6899 - mae: 1.9334 - val_loss: 9.6198 - val_mae: 2.3873\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.0819 - mae: 1.8941 - val_loss: 9.4483 - val_mae: 2.3566\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.4260 - mae: 1.9440 - val_loss: 10.3108 - val_mae: 2.5164\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.5214 - mae: 1.9502 - val_loss: 9.3162 - val_mae: 2.3453\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7636 - mae: 1.8329 - val_loss: 9.8270 - val_mae: 2.4354\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.5563 - mae: 1.8371 - val_loss: 9.8208 - val_mae: 2.4422\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.3475 - mae: 1.8591 - val_loss: 10.7077 - val_mae: 2.5721\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.3533 - mae: 1.8546 - val_loss: 9.9811 - val_mae: 2.4772\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.8231 - mae: 1.8412 - val_loss: 9.6508 - val_mae: 2.4023\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7323 - mae: 1.8280 - val_loss: 9.2784 - val_mae: 2.3547\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.4987 - mae: 1.8163 - val_loss: 9.6928 - val_mae: 2.4178\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 9.2041 - mae: 1.9566 - val_loss: 9.7513 - val_mae: 2.4327\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.5509 - mae: 1.8220 - val_loss: 9.8565 - val_mae: 2.4461\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.5399 - mae: 1.7513 - val_loss: 11.6495 - val_mae: 2.7232\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 9.6964 - mae: 2.1200 - val_loss: 9.3390 - val_mae: 2.3630\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.8553 - mae: 1.6568 - val_loss: 9.4562 - val_mae: 2.3678\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.2654 - mae: 1.7550 - val_loss: 9.4785 - val_mae: 2.3975\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.3829 - mae: 1.7918 - val_loss: 9.6071 - val_mae: 2.4167\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.6728 - mae: 1.9189 - val_loss: 9.5639 - val_mae: 2.4055\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.9579 - mae: 1.7879 - val_loss: 9.4952 - val_mae: 2.3948\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.7429 - mae: 1.7641 - val_loss: 9.2793 - val_mae: 2.3636\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.2018 - mae: 1.7571 - val_loss: 8.7594 - val_mae: 2.2552\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.3929 - mae: 1.8342 - val_loss: 11.2920 - val_mae: 2.6617\n",
            "processing fold # 3 / 4\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 22ms/step - loss: 620.5869 - mae: 23.1130 - val_loss: 484.0084 - val_mae: 20.3879\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 574.9173 - mae: 21.9534 - val_loss: 434.3848 - val_mae: 19.1162\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 495.0994 - mae: 19.9772 - val_loss: 377.2065 - val_mae: 17.6226\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 406.9656 - mae: 18.1093 - val_loss: 316.5802 - val_mae: 15.9388\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 350.9396 - mae: 16.3711 - val_loss: 254.6749 - val_mae: 14.0722\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 255.0993 - mae: 13.8565 - val_loss: 195.0363 - val_mae: 11.9971\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 201.8826 - mae: 11.7467 - val_loss: 141.9896 - val_mae: 9.8694\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 147.3406 - mae: 9.7335 - val_loss: 102.0388 - val_mae: 8.0834\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 113.0392 - mae: 7.9687 - val_loss: 73.7062 - val_mae: 6.7418\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 73.9421 - mae: 6.3530 - val_loss: 55.8461 - val_mae: 5.8744\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 55.8445 - mae: 5.5614 - val_loss: 44.4688 - val_mae: 5.1299\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 39.9860 - mae: 4.7625 - val_loss: 37.3718 - val_mae: 4.5500\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 38.4745 - mae: 4.6705 - val_loss: 33.1640 - val_mae: 4.1724\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 29.5198 - mae: 4.0765 - val_loss: 29.7002 - val_mae: 3.8693\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 24.2335 - mae: 3.6039 - val_loss: 28.2518 - val_mae: 3.7197\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 19.9425 - mae: 3.3353 - val_loss: 26.6937 - val_mae: 3.6121\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 25.8285 - mae: 3.6234 - val_loss: 25.5748 - val_mae: 3.5172\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 17.7751 - mae: 3.2164 - val_loss: 24.5950 - val_mae: 3.4768\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 22.8768 - mae: 3.4650 - val_loss: 23.4094 - val_mae: 3.2875\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 20.9086 - mae: 3.3301 - val_loss: 22.5458 - val_mae: 3.1580\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.3156 - mae: 3.0793 - val_loss: 21.8783 - val_mae: 3.1115\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 18.8706 - mae: 3.1289 - val_loss: 21.1221 - val_mae: 3.0638\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 20.6170 - mae: 3.1250 - val_loss: 20.5386 - val_mae: 2.9508\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 21.2994 - mae: 3.0917 - val_loss: 19.8100 - val_mae: 2.9403\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 17.1010 - mae: 2.8990 - val_loss: 18.8084 - val_mae: 2.8699\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 12.9262 - mae: 2.5484 - val_loss: 19.2731 - val_mae: 2.8583\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 16.0145 - mae: 2.7619 - val_loss: 17.8120 - val_mae: 2.7229\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 16.2446 - mae: 2.6936 - val_loss: 17.5996 - val_mae: 2.7314\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 13.5453 - mae: 2.6174 - val_loss: 17.4612 - val_mae: 2.6723\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 12.2006 - mae: 2.5539 - val_loss: 16.8019 - val_mae: 2.6931\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.3769 - mae: 2.4005 - val_loss: 16.8046 - val_mae: 2.8083\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.5146 - mae: 2.2818 - val_loss: 16.4096 - val_mae: 2.6856\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.8256 - mae: 2.4010 - val_loss: 16.5341 - val_mae: 2.6701\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 11.0068 - mae: 2.4061 - val_loss: 16.2169 - val_mae: 2.6571\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 9.1325 - mae: 2.2136 - val_loss: 16.4123 - val_mae: 2.7830\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.7685 - mae: 2.2587 - val_loss: 15.7968 - val_mae: 2.6641\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.0911 - mae: 2.2588 - val_loss: 15.5392 - val_mae: 2.6784\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.1466 - mae: 2.2463 - val_loss: 15.6195 - val_mae: 2.7092\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 10.1651 - mae: 2.2728 - val_loss: 15.5471 - val_mae: 2.6293\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.5659 - mae: 2.2518 - val_loss: 15.5724 - val_mae: 2.6072\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.8548 - mae: 2.3325 - val_loss: 15.1892 - val_mae: 2.6182\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.2441 - mae: 2.1257 - val_loss: 15.1451 - val_mae: 2.5849\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.7995 - mae: 2.1792 - val_loss: 14.9813 - val_mae: 2.5938\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.5829 - mae: 2.0963 - val_loss: 15.7390 - val_mae: 2.6146\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.7584 - mae: 2.1056 - val_loss: 14.9347 - val_mae: 2.5843\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.7026 - mae: 2.1948 - val_loss: 14.6018 - val_mae: 2.5835\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.4206 - mae: 2.1182 - val_loss: 15.0737 - val_mae: 2.5937\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.0419 - mae: 2.1284 - val_loss: 14.6826 - val_mae: 2.5945\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.5794 - mae: 2.0821 - val_loss: 14.8153 - val_mae: 2.6249\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.5122 - mae: 2.0731 - val_loss: 14.8392 - val_mae: 2.5979\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 10.4505 - mae: 2.2663 - val_loss: 14.8182 - val_mae: 2.6295\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.1818 - mae: 2.1195 - val_loss: 14.6298 - val_mae: 2.6301\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.2122 - mae: 2.1022 - val_loss: 15.0372 - val_mae: 2.6433\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 8.3797 - mae: 2.0537 - val_loss: 15.1384 - val_mae: 2.6425\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.1201 - mae: 1.9550 - val_loss: 14.6744 - val_mae: 2.6261\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 6.9508 - mae: 1.8663 - val_loss: 14.7387 - val_mae: 2.6023\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.2653 - mae: 1.9451 - val_loss: 14.5518 - val_mae: 2.6178\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.2914 - mae: 1.9789 - val_loss: 15.2258 - val_mae: 2.6700\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.3416 - mae: 2.1728 - val_loss: 14.9043 - val_mae: 2.6033\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.6525 - mae: 1.8799 - val_loss: 14.3270 - val_mae: 2.6026\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.9234 - mae: 1.9334 - val_loss: 14.1940 - val_mae: 2.5741\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.2920 - mae: 1.9758 - val_loss: 14.5786 - val_mae: 2.5979\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.7393 - mae: 2.0810 - val_loss: 14.3238 - val_mae: 2.6492\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.5109 - mae: 1.8656 - val_loss: 15.1822 - val_mae: 2.8050\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.7459 - mae: 2.0771 - val_loss: 14.3960 - val_mae: 2.6134\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.1319 - mae: 1.8554 - val_loss: 14.8191 - val_mae: 2.7622\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.1395 - mae: 2.0183 - val_loss: 14.4362 - val_mae: 2.6542\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.0574 - mae: 2.0793 - val_loss: 14.3716 - val_mae: 2.5876\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.3036 - mae: 1.8360 - val_loss: 14.2772 - val_mae: 2.6164\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.8995 - mae: 1.9683 - val_loss: 14.5649 - val_mae: 2.5919\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.4224 - mae: 1.9918 - val_loss: 14.6758 - val_mae: 2.6150\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.0886 - mae: 2.0044 - val_loss: 14.2667 - val_mae: 2.5808\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.9237 - mae: 1.9242 - val_loss: 14.2250 - val_mae: 2.5754\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.1538 - mae: 1.8402 - val_loss: 14.8215 - val_mae: 2.6192\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.9319 - mae: 1.8975 - val_loss: 13.8911 - val_mae: 2.6168\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.8299 - mae: 2.0125 - val_loss: 14.8713 - val_mae: 2.6299\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.0749 - mae: 1.9388 - val_loss: 14.2032 - val_mae: 2.6121\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7792 - mae: 1.9065 - val_loss: 14.0475 - val_mae: 2.6029\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.8046 - mae: 1.8648 - val_loss: 14.1937 - val_mae: 2.5658\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.7984 - mae: 1.9506 - val_loss: 14.5681 - val_mae: 2.5740\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.4207 - mae: 1.9851 - val_loss: 14.0829 - val_mae: 2.5384\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7220 - mae: 1.9436 - val_loss: 14.0647 - val_mae: 2.5491\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.2715 - mae: 1.9469 - val_loss: 14.2649 - val_mae: 2.5892\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.3683 - mae: 1.7408 - val_loss: 14.2737 - val_mae: 2.5505\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.8337 - mae: 1.8918 - val_loss: 14.1987 - val_mae: 2.5626\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 5.5557 - mae: 1.7595 - val_loss: 14.5781 - val_mae: 2.7736\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.2741 - mae: 1.8650 - val_loss: 14.1968 - val_mae: 2.5153\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.1644 - mae: 1.8607 - val_loss: 14.5997 - val_mae: 2.5643\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.6362 - mae: 1.8781 - val_loss: 14.5160 - val_mae: 2.5482\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.8609 - mae: 1.8285 - val_loss: 14.0798 - val_mae: 2.5786\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.8272 - mae: 1.8107 - val_loss: 14.0310 - val_mae: 2.6024\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.9348 - mae: 1.8818 - val_loss: 14.3582 - val_mae: 2.5511\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.4046 - mae: 1.8653 - val_loss: 14.2661 - val_mae: 2.5355\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.7121 - mae: 1.7687 - val_loss: 14.0447 - val_mae: 2.5243\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.0883 - mae: 1.8705 - val_loss: 14.2082 - val_mae: 2.5317\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.8904 - mae: 1.7384 - val_loss: 14.5062 - val_mae: 2.5657\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.3024 - mae: 1.7979 - val_loss: 13.9407 - val_mae: 2.5214\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.8949 - mae: 1.8133 - val_loss: 14.0212 - val_mae: 2.5503\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.4865 - mae: 1.7719 - val_loss: 14.4268 - val_mae: 2.5459\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.2671 - mae: 1.7998 - val_loss: 14.4784 - val_mae: 2.6112\n",
            "processing fold # 4 / 4\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 1s 21ms/step - loss: 547.2776 - mae: 21.7863 - val_loss: 631.7131 - val_mae: 22.9302\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 457.4673 - mae: 19.7042 - val_loss: 573.0060 - val_mae: 21.5670\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 434.9137 - mae: 18.7495 - val_loss: 513.9182 - val_mae: 20.1631\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 362.5764 - mae: 16.5995 - val_loss: 448.3062 - val_mae: 18.5541\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 304.9527 - mae: 14.9726 - val_loss: 377.0207 - val_mae: 16.7642\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 260.0348 - mae: 13.6882 - val_loss: 310.9451 - val_mae: 14.8872\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 207.8664 - mae: 11.9664 - val_loss: 249.6479 - val_mae: 12.8840\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 166.1917 - mae: 10.5278 - val_loss: 197.1287 - val_mae: 10.9968\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 106.8222 - mae: 8.3620 - val_loss: 155.3080 - val_mae: 9.5231\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 91.8536 - mae: 7.5018 - val_loss: 127.0645 - val_mae: 8.6026\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 67.6422 - mae: 6.3964 - val_loss: 103.5451 - val_mae: 7.7558\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 48.7485 - mae: 5.3294 - val_loss: 86.7933 - val_mae: 6.9963\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 37.7327 - mae: 4.7259 - val_loss: 73.3201 - val_mae: 6.3051\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 49.1244 - mae: 4.7458 - val_loss: 64.3162 - val_mae: 5.7853\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 26.9350 - mae: 3.8437 - val_loss: 53.3753 - val_mae: 5.2275\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 30.0516 - mae: 3.8900 - val_loss: 47.1898 - val_mae: 4.8046\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 26.7791 - mae: 3.3707 - val_loss: 41.9420 - val_mae: 4.4694\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 29.7484 - mae: 3.5020 - val_loss: 37.8441 - val_mae: 4.2599\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 18.1451 - mae: 2.8894 - val_loss: 33.0244 - val_mae: 3.9568\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 26.2855 - mae: 3.2430 - val_loss: 30.9725 - val_mae: 3.7915\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 18.8301 - mae: 2.9366 - val_loss: 28.8493 - val_mae: 3.6625\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 13.3351 - mae: 2.6220 - val_loss: 26.8712 - val_mae: 3.5494\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 16.0294 - mae: 2.6055 - val_loss: 26.2443 - val_mae: 3.4343\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 17.0526 - mae: 2.7986 - val_loss: 23.7704 - val_mae: 3.3700\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 14.8811 - mae: 2.6702 - val_loss: 22.8405 - val_mae: 3.3016\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 12.7590 - mae: 2.5372 - val_loss: 22.5787 - val_mae: 3.2077\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 13.8955 - mae: 2.5314 - val_loss: 21.5145 - val_mae: 3.1744\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 15.8244 - mae: 2.6712 - val_loss: 20.1416 - val_mae: 3.1458\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 11.6890 - mae: 2.3687 - val_loss: 19.9808 - val_mae: 3.0409\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 12.3715 - mae: 2.2544 - val_loss: 18.4106 - val_mae: 2.9735\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.8053 - mae: 2.3180 - val_loss: 18.4431 - val_mae: 3.0183\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 11.7942 - mae: 2.3392 - val_loss: 17.8518 - val_mae: 2.9487\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 12.1836 - mae: 2.4370 - val_loss: 17.7758 - val_mae: 2.9595\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.9042 - mae: 2.2868 - val_loss: 17.2609 - val_mae: 2.8998\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 14.2051 - mae: 2.4431 - val_loss: 17.5596 - val_mae: 2.9783\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.4325 - mae: 2.1249 - val_loss: 16.9198 - val_mae: 2.9018\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.3438 - mae: 2.1868 - val_loss: 17.2836 - val_mae: 2.8958\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.4842 - mae: 2.2536 - val_loss: 16.1454 - val_mae: 2.8584\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 11.4475 - mae: 2.2683 - val_loss: 15.9811 - val_mae: 2.8202\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.6846 - mae: 2.0444 - val_loss: 15.7685 - val_mae: 2.8088\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.9262 - mae: 2.1462 - val_loss: 15.4222 - val_mae: 2.8051\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.8317 - mae: 2.2422 - val_loss: 15.4727 - val_mae: 2.7883\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 11.3553 - mae: 2.0626 - val_loss: 15.2136 - val_mae: 2.7513\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 10.2604 - mae: 2.2999 - val_loss: 15.0145 - val_mae: 2.7952\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.0855 - mae: 1.9284 - val_loss: 14.7630 - val_mae: 2.7906\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.4576 - mae: 2.0792 - val_loss: 14.3221 - val_mae: 2.6885\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.8324 - mae: 2.0766 - val_loss: 14.0009 - val_mae: 2.6906\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.3911 - mae: 2.0860 - val_loss: 14.0551 - val_mae: 2.6987\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.9559 - mae: 2.0492 - val_loss: 14.0572 - val_mae: 2.7036\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.2467 - mae: 2.0031 - val_loss: 14.0212 - val_mae: 2.7137\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.5259 - mae: 1.9898 - val_loss: 13.9068 - val_mae: 2.6635\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.3034 - mae: 2.0466 - val_loss: 14.0815 - val_mae: 2.7597\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.1070 - mae: 2.0434 - val_loss: 13.8583 - val_mae: 2.7418\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.6969 - mae: 1.9893 - val_loss: 13.4258 - val_mae: 2.6069\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.3783 - mae: 2.0822 - val_loss: 13.4440 - val_mae: 2.6366\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.3880 - mae: 2.0270 - val_loss: 13.3052 - val_mae: 2.6233\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.6364 - mae: 1.8965 - val_loss: 13.7142 - val_mae: 2.6556\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.1327 - mae: 1.8887 - val_loss: 13.4837 - val_mae: 2.6982\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.3989 - mae: 2.0990 - val_loss: 12.7886 - val_mae: 2.5913\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.9885 - mae: 1.8849 - val_loss: 13.2460 - val_mae: 2.6488\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 8.1039 - mae: 1.9271 - val_loss: 12.9069 - val_mae: 2.5554\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 10.2593 - mae: 2.1700 - val_loss: 12.8862 - val_mae: 2.5495\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.6420 - mae: 1.9515 - val_loss: 13.7547 - val_mae: 2.7802\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 9.2553 - mae: 1.9945 - val_loss: 13.2717 - val_mae: 2.6951\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.2196 - mae: 1.8348 - val_loss: 12.9496 - val_mae: 2.6211\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.8974 - mae: 1.9121 - val_loss: 12.5569 - val_mae: 2.5326\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7951 - mae: 1.8942 - val_loss: 12.6037 - val_mae: 2.5565\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.5673 - mae: 1.8102 - val_loss: 12.6746 - val_mae: 2.5958\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.8084 - mae: 1.8959 - val_loss: 12.8008 - val_mae: 2.5477\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.1728 - mae: 1.8712 - val_loss: 12.6337 - val_mae: 2.5746\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7521 - mae: 1.8316 - val_loss: 12.4962 - val_mae: 2.5200\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.6280 - mae: 1.9957 - val_loss: 12.5776 - val_mae: 2.5978\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.0825 - mae: 1.7973 - val_loss: 12.6118 - val_mae: 2.5388\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.2851 - mae: 1.9020 - val_loss: 12.9538 - val_mae: 2.6755\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.7511 - mae: 1.9112 - val_loss: 12.5933 - val_mae: 2.6311\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.5320 - mae: 1.6943 - val_loss: 12.7815 - val_mae: 2.5623\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.2333 - mae: 2.0134 - val_loss: 12.6272 - val_mae: 2.6472\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.7261 - mae: 1.9779 - val_loss: 12.6965 - val_mae: 2.6580\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.7166 - mae: 1.7510 - val_loss: 12.9145 - val_mae: 2.6261\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.4202 - mae: 1.7912 - val_loss: 12.3050 - val_mae: 2.5047\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 7.5730 - mae: 1.9115 - val_loss: 12.5709 - val_mae: 2.5607\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 9.1608 - mae: 2.0328 - val_loss: 12.3137 - val_mae: 2.5984\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 5.7517 - mae: 1.7961 - val_loss: 12.4656 - val_mae: 2.5887\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.8345 - mae: 1.7196 - val_loss: 12.4984 - val_mae: 2.5656\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 7.2545 - mae: 1.8868 - val_loss: 12.6124 - val_mae: 2.6593\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 4.7477 - mae: 1.6211 - val_loss: 13.8059 - val_mae: 2.7907\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.7138 - mae: 1.8681 - val_loss: 13.0300 - val_mae: 2.6974\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.2192 - mae: 1.8684 - val_loss: 12.3402 - val_mae: 2.5367\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 5.7469 - mae: 1.7430 - val_loss: 12.1269 - val_mae: 2.5437\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 6.2840 - mae: 1.7983 - val_loss: 12.2702 - val_mae: 2.6108\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.0785 - mae: 1.7359 - val_loss: 12.3961 - val_mae: 2.6059\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.2850 - mae: 1.7093 - val_loss: 12.0491 - val_mae: 2.5067\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.1646 - mae: 1.6525 - val_loss: 12.0784 - val_mae: 2.5093\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 8.4783 - mae: 1.8510 - val_loss: 12.1580 - val_mae: 2.5174\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 8.7580 - mae: 1.9524 - val_loss: 11.8133 - val_mae: 2.4763\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 6.3253 - mae: 1.7926 - val_loss: 12.0567 - val_mae: 2.5317\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 7.5463 - mae: 1.8156 - val_loss: 12.1758 - val_mae: 2.5611\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 5.2533 - mae: 1.6690 - val_loss: 12.7994 - val_mae: 2.5613\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.5581 - mae: 1.8229 - val_loss: 12.0365 - val_mae: 2.5650\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 6.9409 - mae: 1.7476 - val_loss: 11.7436 - val_mae: 2.5239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoME81UQL_VC"
      },
      "source": [
        "#Exe. 5 print the overall score of your model and check its average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-OkHBEZRJ4gr",
        "outputId": "9a05921e-2a3c-4451-b29a-67252e2fd8b7"
      },
      "source": [
        "print(\"Overall scores:-\", all_scores)\n",
        "print(\"Average score:-\", np.mean(np.array(all_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall scores:- [1.8804481029510498, 2.6617023944854736, 2.611178159713745, 2.523930311203003]\n",
            "Average score:- 2.419314742088318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ybzziBnMOY_"
      },
      "source": [
        "#Exe. 6 Now training the network a bit longer: 500 epochs.\n",
        "To keep a record of how well the model does at each epoch, we should modify the training loop to save the per-epoch validation score log. Modify the code from the previous\n",
        "exercise in order to save **val_mean_absolute_error** after each epoch for any\n",
        "fold of cross validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqa-CCCqOmVD"
      },
      "source": [
        "import numpy as np\n",
        "# using k=4 iterations\n",
        "k = 4\n",
        "num_val_samples = len( train_data ) // k\n",
        "num_epochs = 500\n",
        "all_scores = []\n",
        "mae_all_history=[]\n",
        "for i in range (k ):\n",
        "  print ('processing fold #', i+1,\"/\",k )\n",
        "  val_data = train_data [i * num_val_samples : ( i + 1) *num_val_samples ]\n",
        "  val_targets = train_targets [i * num_val_samples : (i + 1) *num_val_samples ]\n",
        "  partial_train_data = np.concatenate ([ train_data [: i * num_val_samples ], train_data [( i + 1) * num_val_samples :]] , axis=0)\n",
        "  partial_train_targets = np . concatenate ([ train_targets [: i *num_val_samples ], train_targets [( i + 1) * num_val_samples :]] ,axis =0)\n",
        "  # you should call build_model here\n",
        "  model=build_model()\n",
        "  # you should fit model here\n",
        "  history = model.fit(partial_train_data , partial_train_targets ,validation_data =( val_data , val_targets ) , epochs = num_epochs ,batch_size =1 , verbose =0)\n",
        "  mae_history = history.history ['val_mae']\n",
        "  mae_all_history.append(mae_history)\n",
        "  #the following line is for evaluating the test set\n",
        "  val_mse , val_mae = model.evaluate(val_data, val_targets , verbose=0)\n",
        "  all_scores.append(val_mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY14geq0PA9j"
      },
      "source": [
        "#Exe. 7 Compute the average of the per-epoch mae scores for all folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jacNhAysfPJU"
      },
      "source": [
        "average_mae = np.mean(np.array(mae_all_history),axis=0)\n",
        "average_mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaQv_6csqj3N"
      },
      "source": [
        "#Exe. 8 Plot the average of the per-epoch MAE scores w.r.t epochs changing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EHWLg0uqmCT"
      },
      "source": [
        "x_range = range(1,len(average_mae)+1)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(x_range, average_mae)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax-nOHBVtiKI"
      },
      "source": [
        "#Exe. 9 It may be a little difficult to see the plot, due to scaling issues and relatively high variance.\n",
        "Let’s do the following:\n",
        "- Omit the first 10 data points, which are on a different scale than the rest of the curve.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz8VxnTluf2k"
      },
      "source": [
        "avg_mae_omit=average_mae[10:]\n",
        "x_range = range(1,len(avg_mae_omit)+1)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(x_range, avg_mae_omit)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq9h1GVwuaTS"
      },
      "source": [
        "- Replace each point with an exponential moving average of the previous points,to\n",
        "obtain a smooth curve i.e. **point = previous point∗f actor+point∗(1−factor)**\n",
        "where factor is a value between 0 and 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_VCVjrttwfb"
      },
      "source": [
        "def smooth_curve(points, factor=0.8):\n",
        "  new_points = []\n",
        "  for point in points:\n",
        "    if new_points:\n",
        "      previous = new_points[-1]\n",
        "      new_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      new_points.append(point)\n",
        "  return new_points\n",
        "exp_mae_history = smooth_curve(average_mae[10:])\n",
        "x_range = range(1, len(exp_mae_history) + 1)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(x_range, exp_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmhAA2AJMZEH"
      },
      "source": [
        "According to this plot, you can check\n",
        "when the validation MAE stops improving or becomes worst after Epoch 70 or 80 . Past the special\n",
        "point around 70 or 80 on the graph, the model starts overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzP0lmZ7Ns12"
      },
      "source": [
        "#Exe. 10 After finishing tuning other parameters of the model, train a final production model on all of the training data, with the best parameters, and then look at its performance on the test data from the Exe. 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8O-OO9JWnwk"
      },
      "source": [
        "# Final model using epochs=80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTGEaDTxN_px"
      },
      "source": [
        "# Final model using epochs=80\n",
        "model=build_model()\n",
        "model_hist=model.fit(train_data, train_targets,validation_data=None, epochs =80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW5032d6Tsj9"
      },
      "source": [
        "test_mse , test_mae = model.evaluate(test_data, test_targets , verbose=0)\n",
        "print(\"MSE:-\",test_mse)\n",
        "print(\"MAE:-\",test_mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFCzuMKW3Nk"
      },
      "source": [
        "# Final model by changing the size of hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k5goh0yVCo_"
      },
      "source": [
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  # complete the model here\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation = None)) # As the output is continuous (regression), we are not using activation function\n",
        "  # compile model\n",
        "  model.compile(optimizer ='rmsprop',loss='mean_squared_error',metrics =['mae'])\n",
        "  return model\n",
        "\n",
        "model=build_model()\n",
        "model_hist=model.fit(train_data, train_targets,validation_data=None, epochs =80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iymaya6YLLM"
      },
      "source": [
        "test_mse , test_mae = model.evaluate(test_data, test_targets , verbose=0)\n",
        "print(\"MSE:-\",test_mse)\n",
        "print(\"MAE:-\",test_mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W2dn0ToZraA"
      },
      "source": [
        "Hence, MAE is still around 2.6"
      ]
    }
  ]
}